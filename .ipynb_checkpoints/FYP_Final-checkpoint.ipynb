{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the requirements.txt to see all installations needed\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import math\n",
    "import timeit\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import kerastuner as kt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner import HyperModel\n",
    "\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" FORM - Used Compute a value between 0:1 for the performance of the \n",
    "    team on the games that were provided. Last game having the largest \n",
    "    weight. E.g. 4 games ['W', 'L', 'D', 'W'] \n",
    "    (left being the latest game), they will\n",
    "    have weights [4,3,2,1] and scores [3, 0, 1, 3].\n",
    "    \"\"\"\n",
    "\n",
    "def form(games):\n",
    "\n",
    "    weight = len(games)\n",
    "    score_sum = 0\n",
    "     \n",
    "    for game in games:\n",
    "        if game is 'W':\n",
    "            # If your win add 3, times the weight of the game\n",
    "            score_sum = score_sum + weight*3\n",
    "            # If your draw add 1 point, times the weight of the game\n",
    "        elif game is 'D':\n",
    "            score_sum = score_sum + weight*1\n",
    "            \n",
    "        else:\n",
    "            score_sum = score_sum + 0\n",
    "        \n",
    "        # Diminish the weight for the next game \n",
    "        weight = weight - 1\n",
    "    \n",
    "#     form formula as seen in the research paper\n",
    "    score_sum = score_sum*2/( 3*len(games)*(1+len(games)))\n",
    "    return score_sum\n",
    "\n",
    "\n",
    "def compute_team_streak(df, x):\n",
    "    \n",
    "    # Each one of these records the performance of each team based on location\n",
    "    team_home_streaks = {}\n",
    "    team_away_streaks = {}\n",
    "    for team in np.unique(df[['HomeTeam', 'AwayTeam']]):\n",
    "        team_home_streaks[team] = 0  \n",
    "        team_away_streaks[team] = 0\n",
    "        \n",
    "    home_streaks_home_team = []\n",
    "    home_streaks_away_team = []\n",
    "    \n",
    "    away_streaks_home_team = []\n",
    "    away_streaks_away_team = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        current_row = df.iloc[i]\n",
    "        \n",
    "        home_streaks_home_team.append( team_home_streaks[current_row['HomeTeam']])\n",
    "        home_streaks_away_team.append( team_home_streaks[current_row['AwayTeam']])\n",
    "        \n",
    "        away_streaks_home_team.append( team_away_streaks[current_row['HomeTeam']])\n",
    "        away_streaks_away_team.append( team_away_streaks[current_row['AwayTeam']])\n",
    "        \n",
    "        # k is the number of previous games to consider and passed in as a parameter, \n",
    "        # .iloc[-k:] takes care of only including last k games. \n",
    "        home_last_x = df.iloc[:i+1].loc[(df['HomeTeam'] == current_row['HomeTeam'])].iloc[-x:]\n",
    "        away_last_x = df.iloc[:i+1].loc[(df['AwayTeam'] == current_row['AwayTeam'])].iloc[-x:]\n",
    "        \n",
    "# convert the 'H' seen in the original data to 'W'. Repeat with Draw and Loss for home and away.        \n",
    "        home_last_games = []\n",
    "        for res in home_last_x['FTR']:\n",
    "            if res == 'H':\n",
    "                home_last_games.append('W')\n",
    "            elif res == 'D':\n",
    "                home_last_games.append('D')\n",
    "            else:\n",
    "                home_last_games.append('L')\n",
    "        \n",
    "        away_last_games = []\n",
    "        for res in away_last_x['FTR']:\n",
    "            if res == 'H':\n",
    "                away_last_games.append('L')\n",
    "            elif res == 'D':\n",
    "                away_last_games.append('D')\n",
    "            else:\n",
    "                away_last_games.append('W')\n",
    "                \n",
    "        # compute weighted streak score for the last games\n",
    "\n",
    "        home_updated = form(home_last_games)\n",
    "        away_updated = form(away_last_games)\n",
    "\n",
    "        # update the dictionary\n",
    "        team_home_streaks[ current_row['HomeTeam']] = home_updated\n",
    "        team_away_streaks[ current_row['AwayTeam']] = away_updated\n",
    "   \n",
    "    #   add form calculations to dataframe\n",
    "    df['home_streak_home_team'] = home_streaks_home_team\n",
    "    df['home_streak_away_team'] = home_streaks_away_team \n",
    "    \n",
    "    df['away_streak_home_team'] = away_streaks_home_team\n",
    "    df['away_streak_away_team'] = away_streaks_away_team\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" SOFT RESET the elo score of each team for the next\n",
    "    season. With factor = 2 would give the value between \n",
    "    current score and current mean for all teams\"\"\"\n",
    "\n",
    "# original Soft reset formula\n",
    "# def update_season_elo(current_elo, mean, factor):\n",
    "#     new_elo = (current_elo - mean)/factor  + mean\n",
    "#     return new_elo\n",
    "\n",
    "# updated SOFT ELO reset\n",
    "def update_season_elo(current_elo, mean, factor):    \n",
    "    new_elo = (mean + (factor*current_elo))/(1 + factor)\n",
    "\n",
    "    return new_elo\n",
    "\n",
    "\n",
    "# MARGIN OF VICOTRY added to the Elo Rating.\n",
    "# NOTE: This is not used in the final model.\n",
    "def compute_G( mov, dr):\n",
    "    if mov <= 1:\n",
    "        value = 1\n",
    "    else:\n",
    "        value = 1.7*mov*(2/(2+0.001*dr))\n",
    "        value = math.log( value, 2)\n",
    "        \n",
    "    return value\n",
    "\n",
    "\n",
    "#  Team A vs Team B - calculate the expected probability of Team A winning vs Team B\n",
    "def expected(A, B):\n",
    "    return 1 / (1 + 10 ** ((B - A) / 400))\n",
    "\n",
    "#  Team B vs Team A - calculate the expected probability of Team B winning vs Team B\n",
    "def expectedB(B, A):\n",
    "    return 1 / (1 + 10 ** ((A - B) / 400))\n",
    "\n",
    "# Calculates the new Elo score of the Team. Margin of Victory not currently used. \n",
    "def elo(old, exp, score, k=32, mov=0, dr=300):\n",
    "    return old + k * (score - exp) \n",
    "\n",
    "# Margin of Victory formula (line 43) not included as it produces worse accuracy.\n",
    "#     return old + k * compute_G(mov, dr)*(score - exp) \n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Computer elo scores for each team given a specific season. \n",
    "    \n",
    "    df: is the dataframe containing the season\n",
    "    global_score: is the cummulative elo scores from the previous seasons\n",
    "    avegae: average elo score at the end of previous seaso\n",
    "    initial_k: the initial value of the k parameter\n",
    "    factor: fraction by which k is decreased after each game \n",
    "    \n",
    "    Return:\n",
    "        elo_scores for the end of this season \n",
    "        and updated dataframe. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def compute_elo_scores(df, global_score, average, initial_k, factor):\n",
    "    # If you have never played you get the average score at the end of the previous season\n",
    "\n",
    "    elo_scores = {}\n",
    "    for team in np.unique(df[['HomeTeam', 'AwayTeam']]):  \n",
    "        # If never played\n",
    "        if global_score[team] == -1:\n",
    "            elo_scores[team] = average\n",
    "        else:\n",
    "            elo_scores[team] = global_score[team]\n",
    "    \n",
    "    \n",
    "    \"\"\"k value for each team \n",
    "    that decreases by factor of 'factor' each time the team plays\n",
    "    Starts with initial_k value.\"\"\" \n",
    "    \n",
    "    k_scores = {}\n",
    "    for team in np.unique(df[['HomeTeam', 'AwayTeam']]):\n",
    "        k_scores[team] = initial_k\n",
    "        \n",
    "        \n",
    "    # Placeholders for the resulting columns \n",
    "    elo_at_the_end_of_game_home = []\n",
    "    elo_at_the_end_of_game_away = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        current_row = df.iloc[i] # Take the current row\n",
    "        \n",
    "        # Add the previous scores\n",
    "        elo_at_the_end_of_game_home.append( elo_scores[ current_row['HomeTeam']])\n",
    "        elo_at_the_end_of_game_away.append( elo_scores[ current_row['AwayTeam']])\n",
    "\n",
    "        # depedning on who wins update the score. \n",
    "        if current_row['FTR'] == 'H':\n",
    "            home_score = 1\n",
    "            away_score = 0\n",
    "        elif current_row['FTR'] == 'D':\n",
    "            home_score = 0.5\n",
    "            away_score = 0.5\n",
    "        else:\n",
    "            home_score = 0\n",
    "            away_score = 1\n",
    "         \n",
    "        \n",
    "        # Do the elo computation \n",
    "        home_expected = expected( elo_scores[ current_row['HomeTeam']], elo_scores[ current_row['AwayTeam']])\n",
    "        home_actual_score = elo( elo_scores[ current_row['HomeTeam']], home_expected , home_score, \n",
    "                                k=k_scores[current_row['HomeTeam']] ) #,  mov=mov, dr=300)\n",
    "        \n",
    "        #k_scores[current_row['HomeTeam']]\n",
    "        away_expected = expected( elo_scores[ current_row['AwayTeam']], elo_scores[ current_row['HomeTeam']])\n",
    "        away_actual_score = elo( elo_scores[ current_row['AwayTeam']], away_expected , away_score, \n",
    "                                k=k_scores[current_row['AwayTeam']]) #, mov=mov, dr=300)\n",
    "        \n",
    "        #Update the scores\n",
    "        elo_scores[current_row['HomeTeam']] = home_actual_score\n",
    "        elo_scores[current_row['AwayTeam']] = away_actual_score\n",
    "        \n",
    "        # Decrease the value of k by a factor for each team. \n",
    "        k_scores[current_row['HomeTeam']] = k_scores[current_row['HomeTeam']] - factor*k_scores[current_row['HomeTeam']]\n",
    "        k_scores[current_row['AwayTeam']] = k_scores[current_row['AwayTeam']] - factor*k_scores[current_row['HomeTeam']]\n",
    "        \n",
    "    \n",
    "    # Add back to the dataframe. \n",
    "    df['Home_points_elo'] = elo_at_the_end_of_game_home\n",
    "    df['Away_points_elo'] = elo_at_the_end_of_game_away \n",
    "    \n",
    "    return df, elo_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes a number of different performance metrics\n",
    "def compute_team_points(df):\n",
    "    \n",
    "    # Create a dictionary with all the teams that are playing\n",
    "    # And initialize teams scores to 0.\n",
    "    team_points = {}\n",
    "    team_shots = {}\n",
    "    team_goal_difference = {}\n",
    "    team_shots_faced = {}\n",
    "    team_red = {}\n",
    "    \n",
    "    for team in np.unique(df[['HomeTeam', 'AwayTeam']]):\n",
    "        team_points[team] = 0\n",
    "        team_shots [team] = 0\n",
    "        team_goal_difference [team] = 0\n",
    "        team_shots_faced [team] = 0\n",
    "        team_red[team] = 0\n",
    "    \n",
    "    # The number of games played per team starts with 0.\n",
    "    team_gp_played = {}\n",
    "    for team in np.unique(df[['HomeTeam', 'AwayTeam']]):\n",
    "        team_gp_played[team] = 0   \n",
    "    \n",
    "    # This is the points of the team per game. \n",
    "    team_points_per_game = {}\n",
    "    for team in np.unique(df[['HomeTeam', 'AwayTeam']]):\n",
    "        team_points_per_game[team] = 0\n",
    "    \n",
    "    # Account for the fact that we have 1 column per Team.\n",
    "    points_at_the_end_of_game_home = []\n",
    "    points_at_the_end_of_game_away = []\n",
    "    \n",
    "    # One column per Team.\n",
    "    average_points_home = []\n",
    "    average_points_away = []\n",
    "    \n",
    "    shots_total_home = []\n",
    "    shots_total_away = []\n",
    "    \n",
    "    team_goal_difference_end_home = []\n",
    "    team_goal_difference_end_away = []\n",
    "    \n",
    "    team_red_home = []\n",
    "    team_red_away = []\n",
    "    \n",
    "    team_shots_faced_home = []\n",
    "    team_shots_faced_away = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        current_row = df.iloc[i]\n",
    "        \n",
    "        # First add points from the previous game. \n",
    "        points_at_the_end_of_game_home.append( team_points[current_row['HomeTeam']])\n",
    "        points_at_the_end_of_game_away.append( team_points[current_row['AwayTeam']])\n",
    "            \n",
    "        average_points_home.append( team_points_per_game[current_row['HomeTeam']] )\n",
    "        average_points_away.append( team_points_per_game[current_row['AwayTeam']] )\n",
    "        \n",
    "        # If the home team wins\n",
    "        if current_row['FTR'] == 'H':\n",
    "            # Update team points (3 points go to home, and 0 go to Away)\n",
    "            team_points[current_row['HomeTeam']] = team_points[current_row['HomeTeam']] + 3\n",
    "            team_points[current_row['AwayTeam']] = team_points[current_row['AwayTeam']] + 0\n",
    "        \n",
    "        # If it is a draw each team gets a single point\n",
    "        if current_row['FTR'] == 'D':          \n",
    "            team_points[current_row['HomeTeam']] = team_points[current_row['HomeTeam']] + 1\n",
    "            team_points[current_row['AwayTeam']] = team_points[current_row['AwayTeam']] + 1\n",
    "        \n",
    "        # And if the away team wins it gets the 3 points. \n",
    "        if current_row['FTR'] == 'A':\n",
    "            team_points[current_row['HomeTeam']] = team_points[current_row['HomeTeam']] + 0\n",
    "            team_points[current_row['AwayTeam']] = team_points[current_row['AwayTeam']] + 3\n",
    "            \n",
    "            \n",
    "        # Update number of times played\n",
    "        team_gp_played[current_row['HomeTeam']] = team_gp_played[current_row['HomeTeam']] + 1\n",
    "        team_gp_played[current_row['AwayTeam']] = team_gp_played[current_row['AwayTeam']] + 1\n",
    "            \n",
    "        # Update average points  \n",
    "        team_points_per_game[current_row['HomeTeam']] = team_points[current_row['HomeTeam']] / team_gp_played[current_row['HomeTeam']]\n",
    "        team_points_per_game[current_row['AwayTeam']] = team_points[current_row['AwayTeam']] / team_gp_played[current_row['AwayTeam']]            \n",
    "    \n",
    "        # update the shots average       \n",
    "        shots_total_home.append( team_shots[current_row['HomeTeam']])\n",
    "        shots_total_away.append( team_shots[current_row['AwayTeam']])\n",
    "        team_shots[current_row['HomeTeam']] = (team_shots[current_row['HomeTeam']] + current_row['HS'])/ team_gp_played[current_row['HomeTeam']]\n",
    "        team_shots[current_row['AwayTeam']] = (team_shots[current_row['AwayTeam']] + current_row['AS'])/ team_gp_played[current_row['AwayTeam']] \n",
    "        \n",
    "        # team shots faced average\n",
    "        team_shots_faced_home.append( team_shots[current_row['HomeTeam']])\n",
    "        team_shots_faced_away.append( team_shots[current_row['AwayTeam']])\n",
    "        team_shots_faced[current_row['HomeTeam']] = (team_shots[current_row['HomeTeam']] + current_row['AS'])/ team_gp_played[current_row['HomeTeam']]\n",
    "        team_shots_faced[current_row['AwayTeam']] = (team_shots[current_row['AwayTeam']] + current_row['HS'])/ team_gp_played[current_row['AwayTeam']]\n",
    "                \n",
    "        \n",
    "        # goal difference        \n",
    "        team_goal_difference_end_home.append( team_goal_difference[current_row['HomeTeam']])\n",
    "        team_goal_difference_end_away.append( team_goal_difference[current_row['AwayTeam']])\n",
    "        team_goal_difference[current_row['HomeTeam']] = (team_goal_difference[current_row['HomeTeam']] + current_row['FTHG'] - current_row['FTAG'])\n",
    "        team_goal_difference[current_row['AwayTeam']] = (team_goal_difference[current_row['AwayTeam']] - current_row['FTHG'] + current_row['FTAG'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Add match statistics to the dataset\n",
    "    \n",
    "    \n",
    "    df['H_Shots_Faced'] = team_shots_faced_home\n",
    "    df['A_Shots_Faced'] = team_shots_faced_away\n",
    "    \n",
    "    df['H_Goal_Dif'] = team_goal_difference_end_home\n",
    "    df['A_Goal_Dif'] = team_goal_difference_end_away\n",
    "        \n",
    "    df['H_Shots_Total'] = shots_total_home\n",
    "    df['A_Shots_Total'] = shots_total_away\n",
    "     \n",
    "    df['Home_points'] = points_at_the_end_of_game_home\n",
    "    df['Away_points'] = points_at_the_end_of_game_away \n",
    "\n",
    "    df['Average_home_points'] = average_points_home\n",
    "    df['Average_away_points'] = average_points_away \n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize values to zero mean and deviation of 1\n",
    "def standardize(X):\n",
    "    X = (X - np.mean(X))/np.std(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "# Normalize - put values in between 0 and 1 \n",
    "def normalize(X):\n",
    "    X = (X - np.min(X))/(np.max(X)-np.min(X))\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set inital Elo K value\n",
    "INITIAL_K = 30\n",
    "\n",
    "# Set how much the K value decreases (as a %) after evry game\n",
    "K_FACTOR = 0.04\n",
    "\n",
    "# The number of games included in the form calculation\n",
    "X_STREAK = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting betting odds for late use\n",
    "sef = ['WHH','WHD','WHA']\n",
    "\n",
    "usef_before = ['HomeTeam', 'AwayTeam'] # Info available before the match.\n",
    "usef_after = ['FTHG','FTAG','FTR','HC', 'AC', 'HF', 'AF', 'HY', 'AY', 'HR', 'AR','HS','AS']\n",
    "\n",
    "usef = usef_before + usef_after\n",
    "\n",
    "label = 'FTR'\n",
    "\n",
    "# Take all the files in the dataset\n",
    "database_path = 'archive/20*'\n",
    "\n",
    "# sort them by date\n",
    "filenames = sorted(glob.glob(database_path))\n",
    "\n",
    "# All the teams that ever play\n",
    "all_teams_across_seasons = ['Arsenal', 'Aston Villa', 'Birmingham', 'Blackburn', 'Blackpool',\n",
    "                               'Bolton', 'Bournemouth', 'Brighton', 'Burnley', 'Cardiff',\n",
    "                               'Charlton', 'Chelsea', 'Crystal Palace', 'Derby', 'Everton',\n",
    "                               'Fulham', 'Huddersfield', 'Hull', 'Ipswich', 'Leeds', 'Leicester',\n",
    "                               'Liverpool', 'Man City', 'Man United', 'Middlesbrough',\n",
    "                               'Newcastle', 'Norwich', 'Portsmouth', 'QPR', 'Reading',\n",
    "                               'Sheffield United', 'Southampton', 'Stoke', 'Sunderland',\n",
    "                               'Swansea', 'Tottenham', 'Watford', 'West Brom', 'West Ham',\n",
    "                               'Wigan', 'Wolves']\n",
    "\n",
    "# Initialize the global elo_score for all teams.\n",
    "global_elo_score = {}\n",
    "for team in all_teams_across_seasons:\n",
    "    global_elo_score[team] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive/2000-2001.csv\n",
      "ELO MEAN: 1000.5218593841959\n",
      "archive/2001-2002.csv\n",
      "ELO MEAN: 1008.0093637219346\n",
      "archive/2002-2003.csv\n",
      "ELO MEAN: 1008.1326309309403\n",
      "archive/2003-2004.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 305: expected 57 fields, saw 72\\nSkipping line 306: expected 57 fields, saw 72\\nSkipping line 307: expected 57 fields, saw 72\\nSkipping line 308: expected 57 fields, saw 72\\nSkipping line 309: expected 57 fields, saw 72\\nSkipping line 310: expected 57 fields, saw 72\\nSkipping line 311: expected 57 fields, saw 72\\nSkipping line 312: expected 57 fields, saw 72\\nSkipping line 313: expected 57 fields, saw 72\\nSkipping line 314: expected 57 fields, saw 72\\nSkipping line 315: expected 57 fields, saw 72\\nSkipping line 316: expected 57 fields, saw 72\\nSkipping line 317: expected 57 fields, saw 72\\nSkipping line 318: expected 57 fields, saw 72\\nSkipping line 319: expected 57 fields, saw 72\\nSkipping line 320: expected 57 fields, saw 72\\nSkipping line 321: expected 57 fields, saw 72\\nSkipping line 322: expected 57 fields, saw 72\\nSkipping line 323: expected 57 fields, saw 72\\nSkipping line 324: expected 57 fields, saw 72\\nSkipping line 325: expected 57 fields, saw 72\\nSkipping line 326: expected 57 fields, saw 72\\nSkipping line 327: expected 57 fields, saw 72\\nSkipping line 328: expected 57 fields, saw 72\\nSkipping line 329: expected 57 fields, saw 72\\nSkipping line 330: expected 57 fields, saw 72\\nSkipping line 331: expected 57 fields, saw 72\\nSkipping line 332: expected 57 fields, saw 72\\nSkipping line 333: expected 57 fields, saw 72\\nSkipping line 334: expected 57 fields, saw 72\\nSkipping line 335: expected 57 fields, saw 72\\nSkipping line 336: expected 57 fields, saw 72\\nSkipping line 369: expected 57 fields, saw 62\\nSkipping line 370: expected 57 fields, saw 62\\nSkipping line 371: expected 57 fields, saw 62\\nSkipping line 372: expected 57 fields, saw 62\\nSkipping line 373: expected 57 fields, saw 62\\nSkipping line 374: expected 57 fields, saw 62\\nSkipping line 375: expected 57 fields, saw 62\\nSkipping line 376: expected 57 fields, saw 62\\nSkipping line 377: expected 57 fields, saw 62\\nSkipping line 378: expected 57 fields, saw 62\\nSkipping line 379: expected 57 fields, saw 62\\nSkipping line 380: expected 57 fields, saw 62\\nSkipping line 381: expected 57 fields, saw 62\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELO MEAN: 1015.1915605456122\n",
      "archive/2004-2005.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 337: expected 57 fields, saw 62\\nSkipping line 338: expected 57 fields, saw 62\\nSkipping line 339: expected 57 fields, saw 62\\nSkipping line 340: expected 57 fields, saw 62\\nSkipping line 341: expected 57 fields, saw 62\\nSkipping line 342: expected 57 fields, saw 62\\nSkipping line 343: expected 57 fields, saw 62\\nSkipping line 344: expected 57 fields, saw 62\\nSkipping line 345: expected 57 fields, saw 62\\nSkipping line 346: expected 57 fields, saw 62\\nSkipping line 347: expected 57 fields, saw 62\\nSkipping line 348: expected 57 fields, saw 62\\nSkipping line 349: expected 57 fields, saw 62\\nSkipping line 350: expected 57 fields, saw 62\\nSkipping line 351: expected 57 fields, saw 62\\nSkipping line 352: expected 57 fields, saw 62\\nSkipping line 353: expected 57 fields, saw 62\\nSkipping line 354: expected 57 fields, saw 62\\nSkipping line 355: expected 57 fields, saw 62\\nSkipping line 356: expected 57 fields, saw 62\\nSkipping line 357: expected 57 fields, saw 62\\nSkipping line 358: expected 57 fields, saw 62\\nSkipping line 359: expected 57 fields, saw 62\\nSkipping line 360: expected 57 fields, saw 62\\nSkipping line 361: expected 57 fields, saw 62\\nSkipping line 362: expected 57 fields, saw 62\\nSkipping line 363: expected 57 fields, saw 62\\nSkipping line 364: expected 57 fields, saw 62\\nSkipping line 365: expected 57 fields, saw 62\\nSkipping line 366: expected 57 fields, saw 62\\nSkipping line 367: expected 57 fields, saw 62\\nSkipping line 368: expected 57 fields, saw 62\\nSkipping line 369: expected 57 fields, saw 62\\nSkipping line 370: expected 57 fields, saw 62\\nSkipping line 371: expected 57 fields, saw 62\\nSkipping line 372: expected 57 fields, saw 62\\nSkipping line 373: expected 57 fields, saw 62\\nSkipping line 374: expected 57 fields, saw 62\\nSkipping line 375: expected 57 fields, saw 62\\nSkipping line 376: expected 57 fields, saw 62\\nSkipping line 377: expected 57 fields, saw 62\\nSkipping line 378: expected 57 fields, saw 62\\nSkipping line 379: expected 57 fields, saw 62\\nSkipping line 380: expected 57 fields, saw 62\\nSkipping line 381: expected 57 fields, saw 62\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELO MEAN: 1016.9831040114184\n",
      "archive/2005-2006.csv\n",
      "ELO MEAN: 1016.4125465359151\n",
      "archive/2006-2007.csv\n",
      "ELO MEAN: 1026.4813757599306\n",
      "archive/2007-2008.csv\n",
      "ELO MEAN: 1020.8682041806121\n",
      "archive/2008-2009.csv\n",
      "ELO MEAN: 1026.4942061121023\n",
      "archive/2009-2010.csv\n",
      "ELO MEAN: 1027.915581201587\n",
      "archive/2010-2011.csv\n",
      "ELO MEAN: 1030.2578627330254\n",
      "archive/2011-2012.csv\n",
      "ELO MEAN: 1030.212028930084\n",
      "archive/2012-2013.csv\n",
      "ELO MEAN: 1031.5349999869982\n",
      "archive/2013-2014.csv\n",
      "ELO MEAN: 1033.4609122036777\n",
      "archive/2015-2016.csv\n",
      "ELO MEAN: 1033.7934753463624\n",
      "archive/2016-2017.csv\n",
      "ELO MEAN: 1034.8509921169086\n",
      "archive/2017-2018.csv\n",
      "ELO MEAN: 1040.052393080187\n",
      "archive/2018-2019.csv\n",
      "ELO MEAN: 1037.192722758923\n",
      "archive/2019-2020.csv\n",
      "ELO MEAN: 1039.38621522644\n"
     ]
    }
   ],
   "source": [
    "frames = [] \n",
    "\n",
    "for i, filename in enumerate(filenames):\n",
    "    print(filename)\n",
    "    \"\"\"Prepare full path to the folder. Use the following if on Windows\n",
    "     name = filename.split(\"\\\\\")[0] + '/' + filename.split(\"\\\\\")[1] \"\"\"\n",
    "    \n",
    "    name = filename.split(\"/\")[0] + '/' + filename.split(\"/\")[1] \n",
    "    \n",
    "    # Read csv and extract useful columns\n",
    "    df = pd.read_csv( name, error_bad_lines=False)\n",
    "    \n",
    "    # Take all the useful columns\n",
    "    df = df[usef]\n",
    "    \n",
    "    # Compute team poins for this season \n",
    "    df = compute_team_points(df)\n",
    "    \n",
    "    # Compute team streak for this season \n",
    "    df = compute_team_streak(df, X_STREAK)\n",
    "    \n",
    "    # Compute average elo score\n",
    "    # from the end of last season. \n",
    "    \n",
    "    # If this is first season\n",
    "    if i == 0:\n",
    "        # Start with 1000\n",
    "        global_average = 1000\n",
    "    else:\n",
    "        # Compute the average\n",
    "        global_average = 0\n",
    "        num_of_teams = 0\n",
    "        \n",
    "        for key in global_elo_score.keys():\n",
    "            if global_elo_score[key] != -1:\n",
    "                global_average += global_elo_score[key]\n",
    "                num_of_teams += 1\n",
    "        \n",
    "        # Sum of all points divided by the number of teams\n",
    "        global_average = global_average/num_of_teams\n",
    "    \n",
    "    # Compute elo scores for this season\n",
    "    df, elo_season = compute_elo_scores(df, global_elo_score.copy(), global_average, INITIAL_K, K_FACTOR)\n",
    "    \n",
    "    # Add the name of the season to the dataset \n",
    "    # filename.split('.')[0][-9:] gives  '20xx-20xx'\n",
    "    # where xx are the years\n",
    "    df['Season'] = filename.split('.')[0][-9:]\n",
    "    \n",
    "    # Add the season with updated columns to the final datframe\n",
    "    frames.append( df)\n",
    "    \n",
    "    # Update the GLOBAL elo score ahead of the next season \n",
    "    \n",
    "    # First calculate the average for this season\n",
    "    score_average = 0\n",
    "    for key in elo_season.keys():\n",
    "        score_average += elo_season[key]\n",
    "    score_average = score_average/20\n",
    "    \n",
    "    # print that average\n",
    "    print('ELO MEAN: {}'.format(score_average))\n",
    "    \n",
    "#     SOFT RESET the elo scores for the next season\n",
    "    for key in elo_season.keys():\n",
    "        global_elo_score[key] = update_season_elo(elo_season[key], score_average, 2 )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Season']\n",
    "# X.to_csv (r'C:\\Users\\SeanMcNamara\\Documents\\modelX.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all seasons into a single dataframe\n",
    "X = pd.concat(frames).dropna()\n",
    "\n",
    "# Take the outcome cell\n",
    "y = X['FTR']\n",
    "# Drop it from the X\n",
    "X = X.drop(columns='FTR')\n",
    "\n",
    "# Create ONE-HOT-ENCODING of the Full time result. \n",
    "y_cont = np.zeros((len(y), 3))\n",
    "\n",
    "for i, label in enumerate(y):\n",
    "    if label == 'H':\n",
    "        y_cont[i] = np.array([1,0,0])\n",
    "    elif label == 'D':\n",
    "        y_cont[i] = np.array([0,1,0])\n",
    "    else:\n",
    "        y_cont[i] = np.array([0,0,1])\n",
    "\n",
    "# do not HomeTeam or AwayTeam as a feature\n",
    "X = X.drop(columns='HomeTeam')\n",
    "X = X.drop(columns='AwayTeam')\n",
    "\n",
    "seasons = X['Season']\n",
    "X = X.drop( columns = 'Season')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the test data to use for Google Cloud\n",
    "\n",
    "# z = X[X['Season'].isin(['2018-2019', '2019-2020'])]\n",
    "# z.to_csv (r'C:\\Users\\SeanMcNamara\\Documents\\modelTestNN.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the columns that are going to be used in the model\n",
    "# X_b = X[['Home_points_elo', 'Away_points_elo' , 'home_streak_home_team', 'home_streak_away_team', 'away_streak_home_team', \n",
    "#          'away_streak_away_team',\n",
    "#         'Average_home_points','Average_away_points', 'H_Shots_Total',\n",
    "#          'A_Shots_Total', 'H_Shots_Faced', 'A_Shots_Faced']]\n",
    "X_b = X[['Average_home_points', 'Average_away_points', 'H_Shots_Total', 'A_Shots_Total','H_Shots_Faced','A_Shots_Faced',\n",
    "        'H_Goal_Dif', 'A_Goal_Dif']]\n",
    "\n",
    "X_b= X_b.apply(normalize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate last two seasons for testing\n",
    "\n",
    "separation_point = 0\n",
    "separation_point_2 = 0\n",
    "\n",
    "# take the 2018/2019 and 2019/20 seaason for testing\n",
    "for i in range(len(seasons)):\n",
    "    if seasons.iloc[i] == '2018-2019' and separation_point == 0:\n",
    "        separation_point = i\n",
    "    if seasons.iloc[i] == '2019-2020' and separation_point_2 == 0:\n",
    "        separation_point_2 = i        \n",
    "        \n",
    "X_train = np.copy( X_b[:separation_point])\n",
    "X_test = np.copy( X_b[separation_point:])\n",
    "\n",
    "y_train = np.copy( y_cont[:separation_point])\n",
    "y_test = np.copy( y_cont[separation_point:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model and add limitations to the search e.g. maximum number of neurons\n",
    "# taken from https://keras-team.github.io/keras-tuner/\n",
    "\n",
    "def model_builder(hp):\n",
    "  model = keras.Sequential()\n",
    "  model.add(keras.layers.Flatten(input_shape=(12, )))\n",
    "\n",
    "  # Tune the number of units in the first Dense layer\n",
    "  # Choose an optimal value between 32-512\n",
    "  hp_units = hp.Int('units', min_value=8, max_value=512, step=32)\n",
    "  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "  model.add(keras.layers.Dense(3))\n",
    "\n",
    "  # Tune the learning rate for the optimizer\n",
    "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "  hp_learning_rate = hp.Choice('0.001', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=50,\n",
    "                     factor=3\n",
    "                    )\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=50, validation_data=( X_test, y_test), callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The optimal number of neurons in the first layer is {best_hps.get('units')} \n",
    "and the optimal learning rate \n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "#  find the optimal number of epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, y_train, epochs=25, validation_data=( X_test, y_test))\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 - 3 outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Model 1 - 3 outputs -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer1 (Dense)               (None, 256)               2304      \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 68,867\n",
      "Trainable params: 68,867\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "13/13 [==============================] - 1s 45ms/step - loss: 1.0666 - accuracy: 0.4196 - val_loss: 1.0049 - val_accuracy: 0.4934\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 1.0190 - accuracy: 0.4970 - val_loss: 0.9790 - val_accuracy: 0.5382\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.9998 - accuracy: 0.5083 - val_loss: 0.9699 - val_accuracy: 0.5368\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 1.0038 - accuracy: 0.5053 - val_loss: 0.9657 - val_accuracy: 0.5487\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.9901 - accuracy: 0.5139 - val_loss: 0.9655 - val_accuracy: 0.5395\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.9957 - accuracy: 0.5080 - val_loss: 0.9650 - val_accuracy: 0.5434\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.9957 - accuracy: 0.5129 - val_loss: 0.9630 - val_accuracy: 0.5434\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.9879 - accuracy: 0.5173 - val_loss: 0.9666 - val_accuracy: 0.5395\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.9864 - accuracy: 0.5247 - val_loss: 0.9633 - val_accuracy: 0.5447\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.9886 - accuracy: 0.5197 - val_loss: 0.9640 - val_accuracy: 0.5474\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.9820 - accuracy: 0.5251 - val_loss: 0.9638 - val_accuracy: 0.5526\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.9793 - accuracy: 0.5306 - val_loss: 0.9638 - val_accuracy: 0.5487\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.9813 - accuracy: 0.5303 - val_loss: 0.9623 - val_accuracy: 0.5487\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.9890 - accuracy: 0.5188 - val_loss: 0.9612 - val_accuracy: 0.5461\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.9830 - accuracy: 0.5229 - val_loss: 0.9672 - val_accuracy: 0.5421\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.9865 - accuracy: 0.5192 - val_loss: 0.9643 - val_accuracy: 0.5447\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.9894 - accuracy: 0.5149 - val_loss: 0.9623 - val_accuracy: 0.5474\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.9833 - accuracy: 0.5243 - val_loss: 0.9645 - val_accuracy: 0.5474\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.9846 - accuracy: 0.5247 - val_loss: 0.9638 - val_accuracy: 0.5461\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.9785 - accuracy: 0.5267 - val_loss: 0.9628 - val_accuracy: 0.5434\n"
     ]
    }
   ],
   "source": [
    "# Define the model here\n",
    "\n",
    "layer1 = layers.Dense(256, activation = 'relu', name=\"layer1\", input_shape = (8,))\n",
    "\n",
    "# List of different activation functions\n",
    "# ReLu\n",
    "# SeLu\n",
    "# elu\n",
    "# softmax\n",
    "# softplus\n",
    "\n",
    "# Dropout used after first layer\n",
    "tf.keras.layers.Dropout(.1, input_shape=(2,))\n",
    "\n",
    "layer2 = layers.Dense(256, activation = 'relu', name=\"layer2\")\n",
    "batchnorm2 = layers.BatchNormalization()\n",
    "\n",
    "# layer4 = layers.Dense(216, activation = 'relu', name=\"layer3\" )\n",
    "# layer5 = layers.Dense(216, activation = 'relu', name=\"layer4\" )\n",
    "# layer6 = layers.Dense(216, activation = 'relu', name=\"layer5\" )\n",
    "\n",
    "layer3 = layers.Dense(3, activation = 'softmax', name='output')\n",
    "\n",
    "\n",
    "model_layers = [layer1,  layer2,  layer3]\n",
    "model = keras.Sequential(model_layers)\n",
    "\n",
    "# model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.summary()\n",
    "loss = tf.keras.losses.CategoricalCrossentropy( from_logits = False )\n",
    "\n",
    "# List of different loss functions\n",
    "# loss = tf.keras.losses.Poisson() \n",
    "# loss = tf.keras.losses.KLDivergence()\n",
    "# loss = tf.keras.losses.MeanSquaredError()\n",
    "# loss = tf.keras.losses.MeanSquaredLogarithmicError() \n",
    "# loss = tf.keras.losses.Huber()\n",
    "# loss = tf.keras.losses.Hinge()\n",
    "\n",
    "\n",
    "opt = keras.optimizers.Adam(lr = 0.001)\n",
    "\n",
    "# List of different loss functions\n",
    "# SGD\n",
    "# RMSprop\n",
    "# Adam\n",
    "# Adadelta\n",
    "# Adagrad\n",
    "# Adamax\n",
    "# Nadam\n",
    "# Ftrl\n",
    "\n",
    "\n",
    "model.compile( optimizer=opt, loss = loss, metrics=['accuracy'])\n",
    "history = model.fit( x=X_train, y=y_train, validation_data=( X_test, y_test), batch_size= 512, epochs=20)\n",
    "\n",
    "\n",
    "#model.load_weights( 'path to the folder')\n",
    "#model.save_weights('saved_model/model_with_56_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 - 1 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_2 = np.argmax(y_train, 1)/2\n",
    "y_test_2 = np.argmax(y_test, 1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the tensorflow model here\n",
    "layer1 = layers.Dense(128, activation = 'relu', name=\"layer1\", input_shape = (12,))\n",
    "batchnorm1 = layers.BatchNormalization()\n",
    "layer2 = layers.Dense(256, activation = 'relu', name=\"layer2\")\n",
    "#batchnorm2 = layers.BatchNormalization()\n",
    "# layer4 = layers.Dense(1024, activation = 'relu', name=\"layer3\" )\n",
    "#batchnorm3 = layers.BatchNormalization()\n",
    "layer3 = layers.Dense(1, name='output')\n",
    "\n",
    "\n",
    "model_layers = [layer1,  layer2,  layer3]\n",
    "model = keras.Sequential( model_layers)\n",
    "\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "#model.load_weights( 'path to the folder')\n",
    "model.summary()\n",
    "#loss = tf.keras.losses.CategoricalCrossentropy( from_logits = False )\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile( optimizer=opt, loss = loss)\n",
    "model.fit( x=X_train, y=y_train_2, validation_data=( X_test, y_test_2), batch_size= 32, epochs=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold can be chaged so that the number of draws predicted can be controlled\n",
    "# high threshold means, high number of draws\n",
    "\n",
    "predicted_outcome = [] \n",
    "\n",
    "threshold = 0.0001\n",
    "for prediction in predictions:\n",
    "    if prediction < 0.5 - threshold:\n",
    "        predicted_outcome.append(0)\n",
    "    elif prediction > 0.5 + threshold:\n",
    "        predicted_outcome.append(2)\n",
    "    else:\n",
    "        predicted_outcome.append(1)\n",
    "\n",
    "# get the accuracy of the second model\n",
    "100* accuracy_score( np.argmax(y_test, 1), predicted_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix of training model\n",
    "rowlabel=(\"True:  Home [0]\", \"True: Draw [1]\", \"True: Away[2]\")\n",
    "collabel=(\"Pred:  Home [0]\", \"Pred: Draw [1]\", \"Pred: Away[2]\")\n",
    "\n",
    "\n",
    "y_predicted_train = model.predict(X_train)\n",
    "train_cm = confusion_matrix( np.argmax(y_train,1), np.argmax(y_predicted_train,1), labels = [0, 1, 2])\n",
    "plt.axis('tight')\n",
    "plt.axis('off')\n",
    "plt.title('Training Data')\n",
    "plt.table( cellText= train_cm , colLabels=collabel, rowLabels=rowlabel, loc = 'upper center' , fontsize = 20)\n",
    "plt.show()\n",
    "\n",
    "# confusion matrix of model 2's predictions\n",
    "rowlabel=(\"True:  Home [0]\", \"True: Draw [1]\", \"True: Away[2]\")\n",
    "collabel=(\"Pred:  Home [0]\", \"Pred: Draw [1]\", \"Pred: Away[2]\")\n",
    "\n",
    "y_predicted_train = model.predict(X_train)\n",
    "train_cm = confusion_matrix( np.argmax(y_test,1), predicted_outcome, labels = [0, 1, 2])\n",
    "plt.axis('tight')\n",
    "plt.axis('off')\n",
    "plt.title('Test Data')\n",
    "plt.table( cellText= train_cm , colLabels=collabel, rowLabels=rowlabel, loc = 'upper center' , fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read prediction file made by Google AI platform\n",
    "googlePredictions = pd.read_csv( 'archive/googleCloudPredictions2.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predictions by Google \n",
    "googleP = googlePredictions[['FTR_H_score', 'FTR_A_score' , 'FTR_D_score']]\n",
    "google = np.argmax( np.array(googleP), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_numerical = []\n",
    "\n",
    "# one hot encode the full time result\n",
    "for i, label in enumerate(googlePredictions['FTR']):\n",
    "    if label == 'H':\n",
    "        g_numerical.append(0)\n",
    "    elif label == 'A':\n",
    "        g_numerical.append(1)\n",
    "    else:\n",
    "        g_numerical.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = accuracy_score( g_numerical, google)\n",
    "print('Google AI platform test accuracy: {}'.format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Betting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Predictions by William Hill.\n",
    "\n",
    "betting_frames = []\n",
    "usef = ['WHH','WHD','WHA', 'FTR']\n",
    "labe = 'FTR'\n",
    "\n",
    "for i, filename in enumerate(filenames):\n",
    "    #  if on windows use the following:   \n",
    "    # name = filename.split(\"\\\\\")[0] + '/' + filename.split(\"\\\\\")[1] \n",
    "    name = filename.split(\"/\")[0] + '/' + filename.split(\"/\")[1] \n",
    "    \n",
    "    # Read csv and extract useful columns\n",
    "    df = pd.read_csv( name, error_bad_lines=False)\n",
    "    df = df[usef]\n",
    "    \n",
    "    df['Season'] = filename.split('.')[0][-9:]\n",
    "    betting_frames.append( df)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix of predicted test data for 2017/18 and 2018/19 william hill\n",
    "\n",
    "test_accuracy = accuracy_score( np.argmax(y_test,1), wh_prediction)\n",
    "print('For William Hill test accuracy: {}'.format(test_accuracy))\n",
    "\n",
    "rowlabel=(\"True:  Home [0]\", \"True: Draw [1]\", \"True: Away[2]\")\n",
    "collabel=(\"Pred:  Home [0]\", \"Pred: Draw [1]\", \"Pred: Away[2]\")\n",
    "\n",
    "plt.figure()\n",
    "y_predicted_train = model.predict(X_train)\n",
    "train_cm = confusion_matrix( np.argmax(y_test,1), predicted_outcome, labels = [0, 1, 2])\n",
    "plt.axis('tight')\n",
    "plt.axis('off')\n",
    "plt.title('William Hill Predictions')\n",
    "plt.table( cellText= test_cm , colLabels=collabel, rowLabels=rowlabel, loc = 'upper center' , fontsize = 20)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix of predicted test data for 2017/18 and 2018/19 william hill\n",
    "\n",
    "y_predicted_test = model.predict(X_test)\n",
    "test_cm = confusion_matrix( np.argmax(y_test,1), wh_prediction, labels = [0, 1, 2])\n",
    "\n",
    "# as a percentage\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(test_cm/np.sum(test_cm),annot=True, cmap='Blues', ax = ax, fmt='.2%')\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels: William Hill');ax.set_ylabel('True labels'); \n",
    "ax.xaxis.set_ticklabels(['Home', 'Draw','Away']); ax.yaxis.set_ticklabels(['Home', 'Draw','Away']);\n",
    "\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "ax.xaxis.set_label_position('top')\n",
    "\n",
    "test_accuracy = accuracy_score( np.argmax(y_test,1), wh_prediction)\n",
    "print('For William Hill test accuracy: {}'.format(test_accuracy))\n",
    "# save as png file\n",
    "# plt.savefig('testing.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nan values\n",
    "XB = pd.concat(betting_frames).dropna()\n",
    "\n",
    "# List of seasons that will be considered when computing earnings\n",
    "games_to_consider = ['2018-2019', '2019-2020']\n",
    "XB = pd.concat( [XB[XB['Season'] == x] for x in games_to_consider]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting label to numerical \n",
    "y = XB['FTR']\n",
    "XB = XB.drop(columns='FTR')\n",
    "\n",
    "y_numerical = []\n",
    "\n",
    "for i, label in enumerate(y):\n",
    "    if label == 'H':\n",
    "        y_numerical.append(0)\n",
    "    elif label == 'D':\n",
    "        y_numerical.append(1)\n",
    "    else:\n",
    "        y_numerical.append(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predictions by William and model predictions. \n",
    "wh_pred = XB[['WHH', 'WHD', 'WHA']]\n",
    "wh_prediction = np.argmin( np.array(wh_pred), 1)\n",
    "our_prediction = np.argmax( np.array(y_predicted_test ), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kKelly Criterion - used to estimate how much of bankroll should be spent on a bet\n",
    "def kelly_rating(odds, estimate):\n",
    "    return ( (((odds-1)*(estimate)) - (1-estimate) )/(odds-1) )*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kelly Criterion\n",
    "\n",
    "balance = 1000\n",
    "balance_over_time = []\n",
    "\n",
    "for i in range(len(wh_prediction)):\n",
    "    \n",
    "    # Difference between your and William Hill probability\n",
    "    difference = y_predicted_test[i, our_prediction[i]] - (1/np.array(wh_pred))[i, our_prediction[i]]\n",
    "    \n",
    "    # If the difference is between a cetain margin. \n",
    "    if difference > 0.00 and difference < 0.3:\n",
    "        \n",
    "        # Extract from Kelly the percentage of your balance you want to bet\n",
    "        kelly_percentage = kelly_rating( wh_pred.iloc[i, our_prediction[i]] , y_predicted_test[i, our_prediction[i]] )\n",
    "        amount_to_invest = kelly_percentage * balance / 100\n",
    "        \n",
    "        # Make bet and change your balance based on the outcome. \n",
    "        if wh_prediction[i] == y_numerical[i]:\n",
    "            balance = balance + amount_to_invest*(wh_pred.iloc[i, wh_prediction[i]] - 1)\n",
    "        else:\n",
    "            balance = balance - amount_to_invest\n",
    "        \n",
    "        # Add to balance over time. \n",
    "        balance_over_time.append(balance)\n",
    "        \n",
    "print(balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keely Criterion balance \n",
    "plt.ylabel('Bankroll', labelpad = 10) \n",
    "plt.xlabel('Games', labelpad = 10) \n",
    "\n",
    "plt.xlim(0, 150)\n",
    "plt.plot(balance_over_time, color = 'black')\n",
    "\n",
    "# plt.savefig('kellyCriterion.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betting on the prediction of the model\n",
    "\n",
    "winning = 0\n",
    "winnings = []\n",
    "x = 0\n",
    "# y_numerical is the actual rsult\n",
    "\n",
    "for i in range(len(wh_prediction)):\n",
    "\n",
    "#     model predicting a home win\n",
    "    if predicted_outcome[i] == 0:\n",
    "        if y_numerical[i] == 0:\n",
    "            winning = winning + wh_pred.iloc[i, 0] - 1\n",
    "\n",
    "        else:\n",
    "            winning = winning - 1\n",
    "\n",
    "#     model predicting a draw win\n",
    "    if predicted_outcome[i] == 1:\n",
    "        if y_numerical[i] == 1:\n",
    "            winning = winning + wh_pred.iloc[i, 1] - 1\n",
    "\n",
    "        else:\n",
    "            winning = winning - 1\n",
    "\n",
    "#     model predicting an away win\n",
    "    if predicted_outcome[i] == 2:\n",
    "        if y_numerical[i] == 2:\n",
    "            winning = winning + wh_pred.iloc[i, 2] - 1\n",
    "\n",
    "        else:\n",
    "            winning = winning - 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    winnings.append(winning)\n",
    "    \n",
    "print(winning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel('Profit/Loss', labelpad = 10) \n",
    "plt.xlabel('Games', labelpad = 10) \n",
    "\n",
    "plt.plot(winnings, color = 'black')\n",
    "\n",
    "# plt.savefig('predictionBetting.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betting on the draw\n",
    "\n",
    "winning = 0\n",
    "winnings = []\n",
    "x = 0\n",
    "# y_numerical is the actual rsult\n",
    "\n",
    "for i in range(len(wh_prediction)):\n",
    "\n",
    "    if predicted_outcome[i] == 1:\n",
    "        if y_numerical[i] == 1:\n",
    "            winning = winning + wh_pred.iloc[i, 1] - 1\n",
    "\n",
    "        else:\n",
    "            winning = winning - 1\n",
    "    \n",
    "    winnings.append(winning)\n",
    "    \n",
    "print(winning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(winnings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  If you bet on the favourite using William Hills odds you would have lost 30 over 2 seasons.\n",
    "\n",
    "winning = 0\n",
    "winnings = []\n",
    "for i in range(len(wh_prediction)):\n",
    "    \n",
    "    if wh_prediction[i] == y_numerical[i]:\n",
    "        winning = winning + wh_pred.iloc[i, wh_prediction[i]] - 1\n",
    "    else:\n",
    "        winning = winning - 1\n",
    "        \n",
    "    winnings.append(winning)\n",
    "        \n",
    "print(winning)\n",
    "\n",
    "plt.plot(winnings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winning = 0\n",
    "winnings = []\n",
    "x = 0\n",
    "\n",
    "for i in range(len(wh_prediction)):\n",
    "    \n",
    "# Difference between model probablty and William Hill probability\n",
    "    difference = y_predicted_test[i, our_prediction[i]] - (1/np.array(wh_pred))[i, our_prediction[i]]\n",
    "    \n",
    "    if difference > 0.00 : \n",
    "\n",
    "        if our_prediction[i] == y_numerical[i]:\n",
    "            winning = winning + wh_pred.iloc[i, our_prediction[i]] - 1\n",
    "        \n",
    "        else:\n",
    "            winning = winning - 1\n",
    "            \n",
    "        winnings.append(winning)\n",
    "        \n",
    "print(winning)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(winnings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a bet if William Hill is the same as our prediction\n",
    "\n",
    "winning = 0\n",
    "winnings = []\n",
    "\n",
    "for i in range(len(wh_prediction)):\n",
    "    \n",
    "    if wh_prediction[i] == our_prediction[i]:\n",
    "        if wh_prediction[i] == y_numerical[i]:\n",
    "            winning = winning + wh_pred.iloc[i, wh_prediction[i]] - 1\n",
    "        else:\n",
    "            winning = winning - 1\n",
    "\n",
    "        winnings.append(winning)\n",
    "        \n",
    "print(winning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(winnings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code previously used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Margin of Victory (removed from model after testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_G(mov,dr):\n",
    "    value = 1.7*mov*(2/(2+0.001*dr))\n",
    "    value = math.log( value, 2)\n",
    "\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOV = np.linspace(0,10,11)\n",
    "G = []\n",
    "dr = 300\n",
    "\n",
    "def compute_G(mov,dr):\n",
    "    value = 1.7*mov*(2/(2+0.001*dr))\n",
    "    value = math.log( value, 2)\n",
    "\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mov in MOV:\n",
    "    if mov <= 1:\n",
    "        G.append(1)\n",
    "    else:\n",
    "        value = 1.7*mov*(2/(2+0.001*dr))\n",
    "        value = math.log( value, 2)\n",
    "        G.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot graph and change the dr values\n",
    "\n",
    "plt.plot(MOV, G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Convert Catergorical data into numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: NOT NEEDED AFTER CHANGE TO FEATURES\n",
    "\n",
    "# Separate numeric and categorical values\n",
    "# numeric_data = X.select_dtypes(include=[np.number])\n",
    "# #categorical_data = X.select_dtypes(exclude=[np.number])\n",
    "\n",
    "\n",
    "# #categorical_data = categorical_data.apply(categorical_to_numerical)\n",
    "# numeric_data = numeric_data.apply(standardize)\n",
    "\n",
    "# X = pd.concat([ categorical_data, numeric_data], axis=1)\n",
    "\n",
    "# # Write a function to convert categorical to numerical values\n",
    "# def categorical_to_numerical_for_teams(columns):\n",
    "#     uniqueNames = np.unique(columns)\n",
    "    \n",
    "#     columnNumericalHome = np.zeros(len(columns))\n",
    "#     columnNumericalAway = np.zeros(len(columns))\n",
    "    \n",
    "#     for i, instance in enumerate(columns['HomeTeam']):\n",
    "#         for j in range(len(uniqueNames)):\n",
    "#             if uniqueNames[j] == instance:\n",
    "#                 columnNumericalHome[i] = j\n",
    "                \n",
    "#     for i, instance in enumerate(columns['AwayTeam']):\n",
    "#         for j in range(len(uniqueNames)):\n",
    "#             if uniqueNames[j] == instance:\n",
    "#                 columnNumericalAway[i] = j\n",
    "                \n",
    "#     return columnNumericalHome, columnNumericalAway\n",
    "\n",
    "# # Write a function to convert categorical to numerical values\n",
    "# def categorical_to_numerical(column):\n",
    "#     uniqueNames = np.unique(column)\n",
    "#     columnNumerical = np.zeros(len(column))\n",
    "    \n",
    "#     for i, instance in enumerate(column):\n",
    "#         for j in range(len(uniqueNames)):\n",
    "#             if uniqueNames[j] == instance:\n",
    "#                 columnNumerical[i] = j\n",
    "                \n",
    "#     return columnNumerical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glicko Implementation - inconsistent results (potential bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_glicko_for_RP(df, seasonal_scores):\n",
    "    \n",
    "#     for team in seasonal_scores.keys():\n",
    "#         games = df.loc[ (df['HomeTeam'] == team) | (df['AwayTeam'] == team)]\n",
    "        \n",
    "#         opposing_scores = []\n",
    "#         opposing_deviations = []\n",
    "#         resuls = []\n",
    "        \n",
    "#         # go thorough all the games\n",
    "#         for i in range(len(games_played)):\n",
    "            \n",
    "#             # if the current team is home team \n",
    "#             if games_played.iloc[i]['HomeTeam'] = team:\n",
    "#                 opposing = games_played.iloc[i]['AwayTeam']\n",
    "                \n",
    "#                 if games_played.iloc[i]['FTR'] == 'H':\n",
    "#                     results.append(1)\n",
    "#                 elif games_played.iloc[i]['FTR'] == 'D':\n",
    "#                     results.append(0.5)\n",
    "#                 else:\n",
    "#                     results.append(0)\n",
    "                    \n",
    "#             else:\n",
    "#                 opposing = games_played.iloc[i]['HomeTeam']\n",
    "                \n",
    "#                 if games_played.iloc[i]['FTR'] == 'A':\n",
    "#                     results.append(1)\n",
    "#                 elif games_played.iloc[i]['FTR'] == 'D':\n",
    "#                     results.append(0.5)\n",
    "#                 else:\n",
    "#                     results.append(0)\n",
    "                \n",
    "#             opposing_team = seasonal_scores[opposing]\n",
    "            \n",
    "            \n",
    "#             opposing_scores.append( opposing_team.rating)\n",
    "#             opposing_deviations.append( opposing_team.rd)\n",
    "\n",
    "#         team_object = seasonal_scores[team]\n",
    "#         team_object.update_player( opposing_scores, opposing_deviations, results)\n",
    "#         seasonal_scores[team] = team_objects\n",
    "        \n",
    "#     return seasonal_scores\n",
    "\n",
    "# def compute_glicko_for_season(df, fraction, global_glicko_objects):\n",
    "    \n",
    "#     # Initialize all the teams that are playing this season\n",
    "#     seasonal_glicko_scores = {}\n",
    "#     for team in np.unique(df[['HomeTeam', 'AwayTeam']]):\n",
    "#         if global_glicko_objects[team] is None:\n",
    "#             seasonal_glicko_scores[team] = Player()\n",
    "#         else:\n",
    "#             seasonal_glicko_scores[team] = global_glicko_objects[team]\n",
    "            \n",
    "#     # separate the season into fractions\n",
    "#     F = int(round(len(df)*fraction))\n",
    "    \n",
    "#     i = 0\n",
    "#     while (i*F < len(df)):\n",
    "#         current_period = df.iloc[i*F:(i+1)*F]\n",
    "#         updated_seasonal_scores = computer_glicko_for_RP( current_period, seasonal_glicko_scores)\n",
    "#         seasonal_glicko_scores = updated_seasonal_scores\n",
    "        \n",
    "#         for team in global_glicko_objects:\n",
    "#             if global_glicko_objects[team] is not None:\n",
    "#                 if global_glicko_objects[team] not in np.unique(df[['HomeTeam', 'AwayTeam']]):\n",
    "#                     global_glicko_objects[team] = global_glicko_objects[team].did_not_compete()\n",
    "#         i = i + 1\n",
    "    \n",
    "    \n",
    "    \n",
    "#     return global_glicko_objects\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
